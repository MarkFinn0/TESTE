{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1443276f",
   "metadata": {},
   "source": [
    "# 1.Pré tratamento dos dados para análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "001adca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c2993c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"Cuspidão\\Completao.csv\",delimiter=';',encoding=\"latin-1\",decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e52f9",
   "metadata": {},
   "source": [
    "# Criar variáveis gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "8db85fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria um variável com o nome de todos os setores do dataset e cria uma flag para se tem somente essa empresa no setor\n",
    "ListofCompanies=df.drop_duplicates(subset=['SETOR_ATIV','Nome Empresa'])\n",
    "ListofCompanies=ListofCompanies['SETOR_ATIV'].value_counts()\n",
    "ListofCompanies=pd.DataFrame(ListofCompanies)\n",
    "Listof1Companies=ListofCompanies[ListofCompanies['SETOR_ATIV']<4]\n",
    "ListofCompanies=ListofCompanies[ListofCompanies['SETOR_ATIV']>=4]\n",
    "ListofCompanies['Flag1Company']=0\n",
    "Listof1Companies['Flag1Company']=1\n",
    "ListofCompanies=pd.concat([ListofCompanies,Listof1Companies])\n",
    "ListofCompanies['SETOR_ATIV']=ListofCompanies.index\n",
    "\n",
    "#Cria Variáveis de Cuspir\n",
    "DF_Polyline=pd.DataFrame()\n",
    "DF_Cuspidao=pd.DataFrame()\n",
    "DF_Cuspidao2=pd.DataFrame()\n",
    "DF_Cuspidao_BAU=pd.DataFrame()\n",
    "DF_Cuspidao_RESTO=pd.DataFrame()\n",
    "df_produ_kmeans_REST_ALL=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057cfec9",
   "metadata": {},
   "source": [
    "# Pré tratar dados para modelos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "45d57da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar ano e mes\n",
    "df['Ano']=df['DT_REFER'].apply(str).str[0:4]\n",
    "df['Mes']=df['DT_REFER'].apply(str).str[5:7]\n",
    "#Transformar colunas negativas em positivas\n",
    "df['Passivo Circulante']=df['Passivo Circulante']*-1\n",
    "df['Passivo Total']=df['Passivo Total']*-1\n",
    "#Coloca a Flag de 1 company no dataset central\n",
    "df1 = pd.merge( ListofCompanies,df, left_on='SETOR_ATIV', right_on='SETOR_ATIV')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "4bed0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_produ=df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "69a55555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforma as relações de indicadores para clusterizar em numeros, para o for andar de lado.\n",
    "df_produ[14]=df_produ['Ativo Circulante'].clip(lower=0)\n",
    "df_produ[13]=df_produ['Patrimônio Líquido'].clip(lower=0)\n",
    "df_produ[12]=df_produ['Passivo Circulante'].clip(upper=0)\n",
    "df_produ[11]=df_produ['Patrimônio Líquido'].clip(lower=0)\n",
    "df_produ[10]=df_produ['Passivo Total'].clip(upper=0)\n",
    "df_produ[9]=df_produ['Patrimônio Líquido'].clip(lower=0)\n",
    "df_produ[8]=df_produ['Ativo Total'].clip(lower=0)\n",
    "df_produ[7]=df_produ['Patrimônio Líquido'].clip(lower=0)\n",
    "df_produ[6]=df_produ['Reservas de Lucro'].clip(lower=0)\n",
    "df_produ[5]=df_produ['Receita'].clip(lower=0)\n",
    "df_produ[4]=df_produ['Receita'].clip(lower=0)\n",
    "df_produ[3]=df_produ['Patrimônio Líquido'].clip(lower=0)\n",
    "df_produ[2]=df_produ['Receita'].clip(lower=0)\n",
    "df_produ[1]=df_produ['Lucro/Prejuízo do Período'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1ddd54",
   "metadata": {},
   "source": [
    "# Clusterizar as empresas com dados realizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "8160b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_provisorio=df_produ\n",
    "df_provisorio=df_provisorio[['Nome Empresa',\n",
    "                             'Ticker',\n",
    "                             'SETOR_ATIV',\n",
    "                             'Flag1Company',\n",
    "                             'DT_REFER',\n",
    "                             'Ano',\n",
    "                             'Mes',\n",
    "                             1,2,3,4,5,6,7,8,9,10,11,12,13,14]]\n",
    "#BAU KMEANS\n",
    "for y in range(1,13):\n",
    "    for x in ListofCompanies.index.values:\n",
    "        if y==1:\n",
    "            y1=1\n",
    "            y2=2\n",
    "        else:\n",
    "            y1=y+1\n",
    "            y2=y+2\n",
    "        #filtra o setor\n",
    "        df_produ_kmeans=df_provisorio[(df_provisorio['SETOR_ATIV']==x)&(df_provisorio['Flag1Company']==0)].reset_index(drop=True)\n",
    "      \n",
    "        Tcks_count=df_produ_kmeans.drop_duplicates(subset=['Ticker','DT_REFER'])\n",
    "        Tcks_count=Tcks_count['DT_REFER'].value_counts()\n",
    "        Tcks_count_mean=Tcks_count.mean()\n",
    "        Tcks_count=pd.DataFrame(Tcks_count)\n",
    "        ListOFTRI=Tcks_count[Tcks_count['DT_REFER']>=Tcks_count_mean]\n",
    "        ListOFTRI['DATE']=ListOFTRI.index\n",
    "        AnoMAX=ListOFTRI['DATE'].apply(str).str[0:4].max()\n",
    "        MesMAX=ListOFTRI['DATE'].apply(str).str[5:7].max()\n",
    "\n",
    "        #Filtrar ano e mês (ultimo tri de cada setor)\n",
    "        df_produ_kmeans=df_produ_kmeans[(df_produ_kmeans[\"Ano\"]==AnoMAX)&(df_produ_kmeans[\"Mes\"]==MesMAX)].reset_index(drop=True)      \n",
    "        #Clusteriza as empresas com valores negativos como 0 (ruim)\n",
    "        df_produ_kmeans_REST_NO=df_produ_kmeans[(df_produ_kmeans[y1]==0)|(df_produ_kmeans[y2]==0)].reset_index(drop=True)\n",
    "        df_produ_kmeans_REST_NO[f\"{y1}{y2} Cluster\"]=0\n",
    "        #Separa as empresas que são elegíveis a escalabilidade e a clusterização\n",
    "        df_produ_kmeans=df_produ_kmeans[(df_produ_kmeans[y1]!=0)&(df_produ_kmeans[y2]!=0)].reset_index(drop=True)\n",
    "        df_produ_kmeans=df_produ_kmeans.sort_values(by=[y1,y2]).reset_index(drop=True)\n",
    "        \n",
    "        #se tiver mais de 1 amostra (empresa)\n",
    "        if len(df_produ_kmeans)>=2:\n",
    "            #FAÇO A ESCALABILIDADE COM STANDARD\n",
    "            scaler = StandardScaler() \n",
    "            pca = PCA(n_components = 2) \n",
    "            features= df_produ_kmeans[[y1,y2]]\n",
    "            scaled_df = scaler.fit_transform(features) \n",
    "            normalized_df = normalize(scaled_df) \n",
    "            normalized_df = pd.DataFrame(normalized_df)\n",
    "            #E DEPOIS REDUZO AS DIMENSÕES E CORRELAÇÕES COM O PCA\n",
    "            X_principal = pca.fit_transform(normalized_df) \n",
    "            features = pd.DataFrame(X_principal) \n",
    "            features.columns = ['P1', 'P2']\n",
    "            c=3\n",
    "            if len(features)<3:\n",
    "                c=len(features)\n",
    "                \n",
    "            #DOU UM FIT NO KMEANS\n",
    "            kmeans = KMeans(n_clusters=c, init='random',\n",
    "                        n_init=10,\n",
    "                        max_iter=300,random_state=0)\n",
    "            kmeans.fit(features)\n",
    "            #DPS EU FAÇO A CLUSTERIZAÇÃO\n",
    "            d = KMeans(n_clusters=c, init='random',\n",
    "                        n_init=10,\n",
    "                        max_iter=300,random_state=0).fit_predict(features)\n",
    "            df_produ_kmeans[f\"{y1}{y2} Cluster\"]=d\n",
    "            DF_Cuspidao2=pd.concat([df_produ_kmeans,df_produ_kmeans_REST_NO]).reset_index(drop=True)\n",
    "            DF_Cuspidao_BAU=pd.concat([DF_Cuspidao2,DF_Cuspidao_BAU]).reset_index(drop=True)\n",
    "        else:\n",
    "            DF_Cuspidao2=pd.concat([df_produ_kmeans,df_produ_kmeans_REST_NO]).reset_index(drop=True)\n",
    "            DF_Cuspidao_BAU=pd.concat([DF_Cuspidao2,DF_Cuspidao_BAU]).reset_index(drop=True)\n",
    "    df_provisorio=DF_Cuspidao_BAU\n",
    "    DF_Cuspidao2=pd.DataFrame()\n",
    "    DF_Cuspidao_BAU=pd.DataFrame()\n",
    "    \n",
    "DF_Cuspidao_BAU=df_provisorio\n",
    "DF_Cuspidao=pd.DataFrame()\n",
    "df_provisorio=df_produ\n",
    "df_provisorio=df_provisorio[['Nome Empresa',\n",
    "                             'Ticker',\n",
    "                             'SETOR_ATIV',\n",
    "                             'Flag1Company',\n",
    "                             'DT_REFER',\n",
    "                             'Ano',\n",
    "                             'Mes',\n",
    "                             1,2,3,4,5,6,7,8,9,10,11,12,13,14]]\n",
    "#REJEITADOS KMEANS\n",
    "for y in range(1,13):\n",
    "    if y==1:\n",
    "        y1=1\n",
    "        y2=2\n",
    "    else:\n",
    "        y1=y+1\n",
    "        y2=y+2\n",
    "    #filtra o setor\n",
    "    df_produ_kmeans=df_provisorio[(df_provisorio['Flag1Company']==1)].reset_index(drop=True)\n",
    "    Tcks_count=df_produ_kmeans.drop_duplicates(subset=['Ticker','DT_REFER'])\n",
    "    Tcks_count=Tcks_count['DT_REFER'].value_counts()\n",
    "    Tcks_count_mean=Tcks_count.mean()\n",
    "    Tcks_count=pd.DataFrame(Tcks_count)\n",
    "    ListOFTRI=Tcks_count[Tcks_count['DT_REFER']>=Tcks_count_mean]\n",
    "    ListOFTRI['DATE']=ListOFTRI.index\n",
    "    AnoMAX=ListOFTRI['DATE'].apply(str).str[0:4].max()\n",
    "    MesMAX=ListOFTRI['DATE'].apply(str).str[5:7].max()\n",
    "    #Filtrar ano e mês (ultimo tri de cada setor)\n",
    "    df_produ_kmeans=df_produ_kmeans[(df_produ_kmeans[\"Ano\"]==AnoMAX)&(df_produ_kmeans[\"Mes\"]==MesMAX)].reset_index(drop=True)\n",
    "    #Clusteriza as empresas com valores negativos como 0 (ruim)\n",
    "    df_produ_kmeans_REST_NO=df_produ_kmeans[(df_produ_kmeans[y1]==0)|(df_produ_kmeans[y2]==0)].reset_index(drop=True)\n",
    "    df_produ_kmeans_REST_NO[f\"{y1}{y2} Cluster\"]=0\n",
    "    #Separa as empresas que são elegíveis a escalabilidade e a clusterização\n",
    "    df_produ_kmeans=df_produ_kmeans[(df_produ_kmeans[y1]!=0)&(df_produ_kmeans[y2]!=0)].reset_index(drop=True)\n",
    "    df_produ_kmeans=df_produ_kmeans.sort_values(by=[y1,y2]).reset_index(drop=True)\n",
    "    #se tiver mais de 1 amostra (empresa)\n",
    "    if len(df_produ_kmeans)>=2:\n",
    "        #FAÇO A ESCALABILIDADE COM STANDARD\n",
    "        scaler = StandardScaler() \n",
    "        pca = PCA(n_components = 2) \n",
    "        features= df_produ_kmeans[[y1,y2]]\n",
    "        scaled_df = scaler.fit_transform(features) \n",
    "        normalized_df = normalize(scaled_df) \n",
    "        normalized_df = pd.DataFrame(normalized_df)\n",
    "        #E DEPOIS REDUZO AS DIMENSÕES E CORRELAÇÕES COM O PCA\n",
    "        X_principal = pca.fit_transform(normalized_df) \n",
    "        features = pd.DataFrame(X_principal) \n",
    "        features.columns = ['P1', 'P2']\n",
    "        c=3\n",
    "        if len(features)<3:\n",
    "            c=len(features)\n",
    "        #DOU UM FIT NO KMEANS\n",
    "        kmeans = KMeans(n_clusters=c, init='random',\n",
    "                    n_init=10,\n",
    "                    max_iter=300,random_state=0)\n",
    "        kmeans.fit(features)\n",
    "        #DPS EU FAÇO A CLUSTERIZAÇÃO\n",
    "        d = KMeans(n_clusters=c, init='random',\n",
    "                    n_init=10,\n",
    "                    max_iter=300,random_state=0).fit_predict(features)\n",
    "        df_produ_kmeans[f\"{y1}{y2} Cluster\"]=d\n",
    "        DF_Cuspidao2=pd.concat([df_produ_kmeans,df_produ_kmeans_REST_NO]).reset_index(drop=True)\n",
    "        DF_Cuspidao_RESTO=pd.concat([DF_Cuspidao2,DF_Cuspidao_RESTO]).reset_index(drop=True)\n",
    "    else:\n",
    "        DF_Cuspidao2=pd.concat([df_produ_kmeans,df_produ_kmeans_REST_NO]).reset_index(drop=True)\n",
    "        DF_Cuspidao_RESTO=pd.concat([DF_Cuspidao2,DF_Cuspidao_RESTO]).reset_index(drop=True)\n",
    "    df_provisorio=DF_Cuspidao_RESTO\n",
    "    DF_Cuspidao2=pd.DataFrame()\n",
    "    DF_Cuspidao_RESTO=pd.DataFrame()\n",
    "DF_kmeans=pd.concat([DF_Cuspidao_BAU,df_provisorio]).reset_index(drop=True)\n",
    "DF_kmeans[\"Dados\"]=\"Realizado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "6b3a891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_kmeans=DF_kmeans[['Nome Empresa','Ticker','SETOR_ATIV','DT_REFER',\"Dados\",\n",
    "             1,2,3,4,5,6,7,8,9,10,11,12,13,14,\n",
    "            '12 Cluster',\n",
    "            '34 Cluster',\n",
    "            '56 Cluster',\n",
    "            '78 Cluster',\n",
    "            '910 Cluster',\n",
    "            '1112 Cluster',\n",
    "            '1314 Cluster',\n",
    "             ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8d4dc",
   "metadata": {},
   "source": [
    "# Prever os próximos 4 tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "38755ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separo os indicadores que eu quero prever\n",
    "df_produ[11]=df_produ['Obrigações Fiscais']\n",
    "df_produ[10]=df_produ['Obrigações Sociais e Trabalhistas']\n",
    "df_produ[9]=df_produ['Contas a Receber']\n",
    "df_produ[8]=df_produ['Ativo Circulante']\n",
    "df_produ[7]=df_produ['Passivo Circulante']\n",
    "df_produ[6]=df_produ['Passivo Total']\n",
    "df_produ[5]=df_produ['Ativo Total']\n",
    "df_produ[4]=df_produ['Reservas de Lucro']\n",
    "df_produ[3]=df_produ['Patrimônio Líquido']\n",
    "df_produ[2]=df_produ['Receita']\n",
    "df_produ[1]=df_produ['Lucro/Prejuízo do Período']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "94532911",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_provisorio=df_produ\n",
    "df_provisorio=df_provisorio[['Nome Empresa',\n",
    "                             'Ticker',\n",
    "                             'SETOR_ATIV',\n",
    "                             'Flag1Company',\n",
    "                             'DT_REFER',\n",
    "                             'Ano',\n",
    "                             'Mes',\n",
    "                             1,2,3,4,5,6,7,8,9,10,11]]\n",
    "\n",
    "#pego a lista de empresas para analisar\n",
    "LitsofTickers=df_provisorio.drop_duplicates(subset='Ticker')\n",
    "LitsofTickers=LitsofTickers['Ticker'].values\n",
    "for x in LitsofTickers:\n",
    "    DF_Poly=df_provisorio[df_provisorio['Ticker']==x].reset_index(drop=True)\n",
    "    DF_Poly.sort_values(by=['DT_REFER']).reset_index(drop=True)\n",
    "    #transformo os indexes em variáveis e a data em datetime\n",
    "    DF_Poly[\"indexes\"]=DF_Poly.index\n",
    "    DF_Poly['DT_REFER']=pd.to_datetime(DF_Poly['DT_REFER'])\n",
    "    #pego a ultima linha dessa empresa para pegar as informações principais para criar um novo dataset\n",
    "    Lastmonth=DF_Poly.iloc[-1]['DT_REFER']\n",
    "    Lastname=DF_Poly.iloc[-1]['Nome Empresa']\n",
    "    Lastsector=DF_Poly.iloc[-1]['SETOR_ATIV']\n",
    "    Lastnamesub=DF_Poly.iloc[-1]['Ticker']\n",
    "    Lastindex=DF_Poly.iloc[-1]['indexes']\n",
    "    #Crio as datas que eu quero ver la na frente,baseado na data do ultimo balanço\n",
    "    Lastmonth1=Lastmonth+ relativedelta(months=+3)\n",
    "    Lastmonth2=Lastmonth+ relativedelta(months=+6)\n",
    "    Lastmonth3=Lastmonth+ relativedelta(months=+9)\n",
    "    Lastmonth4=Lastmonth+ relativedelta(months=+12)\n",
    "    #Crio um dataframe novo com essas datas\n",
    "    Dateto=pd.DataFrame({'Data_REF': [Lastmonth1,Lastmonth2,Lastmonth3,Lastmonth4]})\n",
    "    X_seq=pd.DataFrame({'x': [Lastindex+1,Lastindex+2,Lastindex+3,Lastindex+4]})\n",
    "    #Configuro o modelo polimonial\n",
    "    polyreg=make_pipeline(PolynomialFeatures(3),LinearRegression())\n",
    "    Pred=Dateto\n",
    "    Pred[\"Nome Empresa\"]=Lastname\n",
    "    Pred['Ticker']=Lastnamesub\n",
    "    Pred['SETOR_ATIV']=Lastsector\n",
    "    #Leio o historico de todos os indicadores e faço a previsão\n",
    "    for y in range(1,12):\n",
    "        yy=DF_Poly[['indexes']]\n",
    "        xx=DF_Poly[[y]]\n",
    "        polyreg.fit(yy,xx)\n",
    "        Pred2=polyreg.predict(X_seq)\n",
    "        Pred2=pd.DataFrame(Pred2,columns=[y])\n",
    "        Pred[y]=Pred2\n",
    "    DF_Polyline=pd.concat([Pred,DF_Polyline]).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "DF_Polyline['Ano']=DF_Polyline['Data_REF'].apply(str).str[0:4]\n",
    "DF_Polyline['Mes']=DF_Polyline['Data_REF'].apply(str).str[5:7]\n",
    "DF_Polyline['Dados']=\"Previsão\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7b6ef5",
   "metadata": {},
   "source": [
    "# Organizar os dados para cuspir no Front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "934c0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_kmeans=DF_kmeans.fillna(0)\n",
    "#Separo as colunas principais do dataset Kmeans e dou um replace nas clusterizações para criar uma nota\n",
    "DF_kmeans_2=DF_kmeans.loc[:,('Ticker','DT_REFER',\"Dados\",'12 Cluster',\n",
    "            '34 Cluster',\n",
    "            '56 Cluster',\n",
    "            '78 Cluster',\n",
    "            '910 Cluster',\n",
    "            '1112 Cluster',\n",
    "            '1314 Cluster')].replace([0,1,2],[0,10,5])\n",
    "#Crio a nota da empresa baseada nos clusters (0 é ruim, 1 é bom e 2 é mediano)\n",
    "DF_kmeans_2['Nota da Empresa']=DF_kmeans_2['12 Cluster']+DF_kmeans_2['34 Cluster']+DF_kmeans_2['56 Cluster']+DF_kmeans_2['78 Cluster']+DF_kmeans_2['910 Cluster']+DF_kmeans_2['1112 Cluster']+DF_kmeans_2['1314 Cluster']\n",
    "DF_kmeans_2['Nota da Empresa']=DF_kmeans_2['Nota da Empresa']/7\n",
    "DF_kmeans_2['Desempenho do Trimestre']=np.where(DF_kmeans_2['Nota da Empresa']<5,\"Ruim\",\n",
    "                                        (np.where((DF_kmeans_2['Nota da Empresa']>=5) & (DF_kmeans_2['Nota da Empresa']<7) ,\"Médio\",\"Bom\")))\n",
    "#Pego o indicador com a melhor nota de clusterização (indicador denominador)\n",
    "DF_kmeans_2['Melhor Indicador1']=(DF_kmeans_2['12 Cluster']+DF_kmeans_2['34 Cluster']+DF_kmeans_2['56 Cluster'])/3\n",
    "DF_kmeans_2['Melhor Indicador2']=(DF_kmeans_2['34 Cluster']+DF_kmeans_2['78 Cluster']+DF_kmeans_2['910 Cluster']+DF_kmeans_2['1112 Cluster']+DF_kmeans_2['1314 Cluster'])/5\n",
    "DF_kmeans_2['Melhor Indicador']=np.where(DF_kmeans_2['Melhor Indicador1']>DF_kmeans_2['Melhor Indicador2'],\"Receita\",\n",
    "                                        (np.where(DF_kmeans_2['Melhor Indicador1']<DF_kmeans_2['Melhor Indicador2'],\"Patrimônio Líquido\",\"Nenhum\")))\n",
    "df_produ=df1\n",
    "#Pego as colunas antigas sem tratamento de clips\n",
    "DF_kmeans_2 = pd.merge(df_produ,DF_kmeans_2 ,left_on=['Ticker','DT_REFER'], right_on=['Ticker','DT_REFER'])\n",
    "DF_kmeans_2['Analise']=DF_kmeans_2['Flag1Company'].replace([0,1],[\"Analise por Setor\",'Analise com outras empresas'])\n",
    "DF_kmeans_2['Tipo de Analise']='Kmeans'\n",
    "#Deixo tudo limpinho e arrumado no dataset kmeans\n",
    "DF_kmeans_2=DF_kmeans_2[['Nome Empresa','Ticker','SETOR_ATIV','Dados','Analise','Tipo de Analise','Melhor Indicador','Desempenho do Trimestre','DT_REFER','Lucro/Prejuízo do Período','Ativo Total','Ativo Circulante','Contas a Receber','Passivo Total','Passivo Circulante','Obrigações Sociais e Trabalhistas','Obrigações Fiscais','Reservas de Lucro','Patrimônio Líquido','Receita']]\n",
    "#Dou nome aos bois no dataset do polimonial\n",
    "DF_Polyline_2=DF_Polyline.rename({'Data_REF': 'DT_REFER', \n",
    "                                  1: 'Lucro/Prejuízo do Período',\n",
    "                                  2: 'Receita', \n",
    "                                  3: 'Patrimônio Líquido', \n",
    "                                  4: 'Reservas de Lucro', \n",
    "                                  5: 'Ativo Total',\n",
    "                                  6: 'Passivo Total', \n",
    "                                  7: 'Passivo Circulante', \n",
    "                                  8: 'Ativo Circulante',\n",
    "                                  9: 'Contas a Receber',\n",
    "                                  10: 'Obrigações Sociais e Trabalhistas',\n",
    "                                  11: 'Obrigações Fiscais',\n",
    "                                 }, axis=1)\n",
    "#Seto as mesmas colunas do kmeans\n",
    "DF_Polyline_2['Melhor Indicador']='Não Analisado'\n",
    "DF_Polyline_2['Desempenho do Trimestre']='Não Analisado'\n",
    "DF_Polyline_2['Nota da Empresa']='Não Analisado'\n",
    "DF_Polyline_2['Analise']='Analise de Historico Financeiro'\n",
    "DF_Polyline_2['Tipo de Analise']='Regressão Polimonial'\n",
    "#Deixo tudo limpinho e arrumado no dataset polimonial\n",
    "DF_Polyline_2=DF_Polyline_2[['Nome Empresa','Ticker','SETOR_ATIV','Dados','Analise','Tipo de Analise','Melhor Indicador','Desempenho do Trimestre','DT_REFER','Lucro/Prejuízo do Período','Ativo Total','Ativo Circulante','Contas a Receber','Passivo Total','Passivo Circulante','Obrigações Sociais e Trabalhistas','Obrigações Fiscais','Reservas de Lucro','Patrimônio Líquido','Receita']]\n",
    "#Deipois junto tudo e cuspo\n",
    "Base_Unica=pd.concat([DF_Polyline_2,DF_kmeans_2]).reset_index(drop=True)\n",
    "Base_Unica.to_csv(rf\"Cuspidão\\Base_Unica.csv\",decimal=',',sep=';', encoding='latin-1',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "7eab1dda",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (4231324964.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\marqu\\AppData\\Local\\Temp\\ipykernel_12956\\4231324964.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    C:\\Users\\marqu\\Desktop\\Marcus\\PROJETO\\theras_app\\asset\\Empresas_data\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "C:\\Users\\marqu\\Desktop\\Marcus\\PROJETO\\theras_app\\asset\\Empresas_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebccd095",
   "metadata": {},
   "source": [
    "# Código do vitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec7d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6876e325",
   "metadata": {},
   "source": [
    "# Cruzar bases e ter uma base unica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d6692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8244e1f",
   "metadata": {},
   "source": [
    "# PIPELINE CUSPIDOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cbc7fd",
   "metadata": {},
   "source": [
    "# Ajustando setor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cd261fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ef68bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "setor_agrup=pd.read_csv(fr\"Depara\\setor_ajust.csv\",delimiter=';',encoding=\"utf-8\",decimal='.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0cdb1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precisa multiplicar os valores da empresa\n",
    "#Base_Unica['Patrimônio Líquido']=Base_Unica['Patrimônio Líquido']*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "49a0e7f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Base_Unica' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26272\\3244683961.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mBase_Unica_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mBase_Unica\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msetor_agrup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SETOR_ATIV'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SETOR_ASIS'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mBase_Unica_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBase_Unica_json\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SETOR_ATIV'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'SETOR_ASIS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Base_Unica' is not defined"
     ]
    }
   ],
   "source": [
    "Base_Unica_json = pd.merge( Base_Unica, setor_agrup, left_on='SETOR_ATIV', right_on='SETOR_ASIS')\n",
    "Base_Unica_json = Base_Unica_json.drop(columns=['SETOR_ATIV','SETOR_ASIS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a93aa",
   "metadata": {},
   "source": [
    "# criar arquivo unico de leitura dos cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d5cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "640940e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Base_Unica_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26272\\1705753797.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mBase_Unica_CARDS_JSON\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBase_Unica_json\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBase_Unica_json\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Dados'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'Realizado'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mBase_Unica_CARDS_JSON\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBase_Unica_CARDS_JSON\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Ticker'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Nome Empresa'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Patrimônio Líquido'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'SETOR_AJUST'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Desempenho do Trimestre'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DT_REFER'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Base_Unica_json' is not defined"
     ]
    }
   ],
   "source": [
    "Base_Unica_CARDS_JSON=Base_Unica_json[Base_Unica_json['Dados']=='Realizado'].reset_index(drop=True)\n",
    "Base_Unica_CARDS_JSON = Base_Unica_CARDS_JSON.groupby(['Ticker','Nome Empresa','Patrimônio Líquido','SETOR_AJUST','Desempenho do Trimestre'])['DT_REFER'].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "52a5761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria a coluna PL do card\n",
    "def formatar_valor(valor):\n",
    "    if valor >= 1000000000:  # Valor em bilhões\n",
    "        valor_dividido = valor / 1000000000\n",
    "        return f'{round(valor_dividido,2):.2f} BI'\n",
    "    \n",
    "    elif valor >= 1000000:  # Valor em milhões\n",
    "        valor_dividido = valor / 1000000\n",
    "        return f'{round(valor_dividido,2):.2f} MM'\n",
    "    \n",
    "    elif valor >= 1000: # Valor menor que 1 milhão\n",
    "        valor_dividido = valor / 1000\n",
    "        return f'{round(valor_dividido,2):.2f} Mil'\n",
    "    \n",
    "    elif valor < 1000 and valor > -1000000: # Valor menor que 1 milhão\n",
    "        valor_dividido = valor / 1000 \n",
    "        return f'{round(valor_dividido,2):.2f} Mil'\n",
    "    \n",
    "    if valor <= -1000000000:  # Valor em bilhões\n",
    "        valor_dividido = valor / 1000000000\n",
    "        return f'{round(valor_dividido,2):.2f} BI'\n",
    "    \n",
    "    elif valor <= -1000000:  # Valor em milhões\n",
    "        valor_dividido = valor / 1000000\n",
    "        return f'{round(valor_dividido,2):.2f} MM'\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "34b458c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Base_Unica_CARDS_JSON' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26272\\870921924.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mBase_Unica_CARDS_JSON\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PL'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBase_Unica_CARDS_JSON\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Patrimônio Líquido'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatar_valor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Base_Unica_CARDS_JSON' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    \n",
    "Base_Unica_CARDS_JSON['PL'] = Base_Unica_CARDS_JSON ['Patrimônio Líquido'].apply(formatar_valor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "72371201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cria a coluna symbol, PM, gain e CMP\n",
    "#coluna PM depende do Preço médio\n",
    "#coluna symbol:down,side e up\n",
    "#coluna CMP: YoY e MoM\n",
    "\n",
    "Base_Unica_CARDS_JSON['symbol'] = \"side\"\n",
    "Base_Unica_CARDS_JSON['PM'] = \"0.0\"\n",
    "Base_Unica_CARDS_JSON['gain'] = \"0%\"\n",
    "Base_Unica_CARDS_JSON['CMP'] = \"YoY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "0b06e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cria a coluna color\n",
    "\n",
    "def map_desempenho(valor):\n",
    "    if valor == 'Bom':\n",
    "        return 'green'\n",
    "    elif valor == 'Médio':\n",
    "        return 'orange'\n",
    "    elif valor == 'Ruim':\n",
    "        return 'red'\n",
    "    else:\n",
    "        return valor\n",
    "\n",
    "Base_Unica_CARDS_JSON['Desempenho do Trimestre'] = Base_Unica_CARDS_JSON['Desempenho do Trimestre'].apply(map_desempenho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "d31be6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#faz os ajustes finais\n",
    "Base_Unica_CARDS_JSON = Base_Unica_CARDS_JSON.rename(columns={'Ticker':'tick',\n",
    "                                                              'Nome Empresa':'nome',\n",
    "                                                             'SETOR_AJUST':'sector',\n",
    "                                                             'Desempenho do Trimestre':'color'\n",
    "                                                             })\n",
    "Base_Unica_CARDS_JSON=Base_Unica_CARDS_JSON.drop(columns=['Patrimônio Líquido','DT_REFER'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5aa437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "e1dacc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extrai a parte específica da string usando uma expressão regular\n",
    "Base_Unica_CARDS_JSON['nome'] = Base_Unica_CARDS_JSON['nome'].str.replace(' - EM RECUPERAÇÃO JUDICIAL', '')\n",
    "Base_Unica_CARDS_JSON['nome'] = Base_Unica_CARDS_JSON['nome'].str.replace('S.A.', '')\n",
    "Base_Unica_CARDS_JSON['nome'] = Base_Unica_CARDS_JSON['nome'].str.replace('SA.', '')\n",
    "Base_Unica_CARDS_JSON['nome'] = Base_Unica_CARDS_JSON['nome'].str.replace('S/A.', '')\n",
    "Base_Unica_CARDS_JSON['nome'] = Base_Unica_CARDS_JSON['nome'].str.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7b16a9",
   "metadata": {},
   "source": [
    "# Salva os Jsons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "b786b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Converta o DataFrame selecionado em uma lista de dicionários\n",
    "dados = Base_Unica_CARDS_JSON.to_dict('records')\n",
    "\n",
    "# Salve os dados como um arquivo JSON\n",
    "\n",
    "with open('pipeline/cards_main.json', 'w', encoding='utf-8') as arquivo:\n",
    "    json.dump(dados, arquivo, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196a085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5798cadc",
   "metadata": {},
   "source": [
    "# Pipeline tela unica empresa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2070f674",
   "metadata": {},
   "source": [
    "### cuspir informações de finanças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f59d426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_company=pd.read_csv(fr\"Cuspidão\\cuspidor_inf.csv\",delimiter=';',encoding=\"latin-1\",decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bf93a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_company['tick_resume']=inf_company['tick'].str.slice(0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c8843123",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickador=inf_company['tick_resume'].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "for x in tickador:\n",
    "    data=inf_company[inf_company['tick_resume']==x]\n",
    "    \n",
    "    #Pego o PL médio\n",
    "    PL=data[data['Info']=='P/L']\n",
    "    PL=PL['Data'].str.replace('.', '').str.replace(',', '.')\n",
    "    PL=pd.to_numeric(PL).mean()\n",
    "    \n",
    "    #Pego o DivYield\n",
    "    divyield=data[data['Info']=='Div. Yield']\n",
    "    divyield=divyield['Data'].str.replace('.', '').str.replace(',', '.').str.replace('%', '')\n",
    "    divyield=pd.to_numeric(divyield).mean()\n",
    "    \n",
    "    #Pego o ROE\n",
    "    ROE=data[data['Info']=='ROE']\n",
    "    ROE=ROE['Data'].str.replace('.', '').str.replace(',', '.').str.replace('%', '')\n",
    "    ROE=pd.to_numeric(ROE).mean()\n",
    "    \n",
    "    #Pego a cresc.receita\n",
    "    CR5=data[data['Info']=='Cres. Rec (5a)']\n",
    "    CR5=CR5['Data'].str.replace('.', '').str.replace(',', '.').str.replace('%', '')\n",
    "    CR5=pd.to_numeric(CR5).mean()\n",
    "    \n",
    "    #Pego a LPA\n",
    "    LPA=data[data['Info']=='LPA']\n",
    "    LPA=LPA['Data'].str.replace('.', '').str.replace(',', '.')\n",
    "    LPA=pd.to_numeric(LPA).mean()\n",
    "    \n",
    "    #Pego a Lucro.Liq\n",
    "    LucroLiqu=data[data['Info']=='Relatorio 3 meses Lucro Líquido']\n",
    "    LucroLiqu=LucroLiqu['Data'].str.replace('.', '').str.replace(',', '.')\n",
    "    LucroLiqu=pd.to_numeric(LucroLiqu).mean()\n",
    "    LucroLiqu=formatar_valor(LucroLiqu)\n",
    "    \n",
    "    #Pego a Lucro.Liq\n",
    "    Valormercado=data[data['Info']=='Valor de mercado']\n",
    "    Valormercado=Valormercado['Data'].str.replace('.', '').str.replace(',', '.')\n",
    "    Valormercado=pd.to_numeric(Valormercado).mean()\n",
    "    Valormercado=formatar_valor(Valormercado)\n",
    "    \n",
    "    \n",
    "    #Pego a data ulti.balan\n",
    "    UBL=data[data['Info']=='Últ balanço processado']\n",
    "    UBL=UBL['Data']\n",
    "    # Faz o parsing da data no formato \"01/01/2023\"\n",
    "    UBL= datetime.strptime(UBL.iloc[0], '%d/%m/%Y')   \n",
    "    # Formata a data no formato \"Jan-23\"\n",
    "    UBL = UBL.strftime('%b-%y')\n",
    "\n",
    "    #Pego a MENOR COTAÇÃO\n",
    "    MIN=data[data['Info']=='Min 52 sem']\n",
    "    MIN=MIN['Data'].str.replace('.', '').str.replace(',', '.')\n",
    "    MIN=pd.to_numeric(MIN).mean()\n",
    "    \n",
    "    #Pego a MAIOR COTAÇÃO\n",
    "    MAX=data[data['Info']=='Max 52 sem']\n",
    "    MAX=MAX['Data'].str.replace('.', '').str.replace(',', '.')\n",
    "    MAX=pd.to_numeric(MAX).mean()\n",
    "\n",
    "    new_row = pd.DataFrame({\"tick\": x, \n",
    "                            \"DY\":f\"{round(divyield,2)}%\", \n",
    "                            \"PL\":f\"{round(PL,2)}\",\n",
    "                            \"ROE\":f\"{round(ROE, 2)}%\",\n",
    "                            \"CR5\":f\"{round(CR5,2)}%\",\n",
    "                            \"LPA\":f\"{round(LPA,2)}\",\n",
    "                            \"LucroLiqu\":LucroLiqu,\n",
    "                            \"Valormercado\":Valormercado,\n",
    "                            \"UBL\":UBL,\n",
    "                            \"MIN\":f\"{round(MIN,2)}\",\n",
    "                            \"MAX\":f\"{round(MAX,2)}\"}, index=[0])\n",
    "    # Converta o DataFrame selecionado em uma lista de dicionários\n",
    "    dados = new_row.to_dict('records')\n",
    "\n",
    "    # Salve os dados como um arquivo JSON\n",
    "\n",
    "    with open(f'pipeline/Empresas_data/{x}_fundamentalist.json', 'w', encoding='utf-8') as arquivo:\n",
    "        json.dump(dados, arquivo, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0293b294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mar-23'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561297f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
